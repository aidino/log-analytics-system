{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"pyspark-notebook\").\\\n",
    "config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|97.86.147.130 - - [15/Aug/2023:15:32:09 +0700] \"POST /niches HTTP/1.0\" 405 3170                                              |\n",
      "|162.176.171.13 - prohaska5212 [15/Aug/2023:15:32:09 +0700] \"POST /ubiquitous/empower/content/roi HTTP/1.1\" 502 10984         |\n",
      "|70.109.63.88 - sauer7026 [15/Aug/2023:15:32:09 +0700] \"DELETE /reinvent/innovative HTTP/1.0\" 304 5075                        |\n",
      "|237.80.86.112 - okuneva7881 [15/Aug/2023:15:32:09 +0700] \"PUT /target HTTP/1.1\" 100 24483                                    |\n",
      "|198.91.2.115 - strosin2660 [15/Aug/2023:15:32:09 +0700] \"HEAD /infrastructures HTTP/2.0\" 503 24834                           |\n",
      "|37.79.127.83 - nitzsche4178 [15/Aug/2023:15:32:09 +0700] \"GET /open-source/scale/synergies/engage HTTP/2.0\" 500 11962        |\n",
      "|28.59.248.3 - - [15/Aug/2023:15:32:09 +0700] \"HEAD /maximize/b2c HTTP/1.1\" 302 20227                                         |\n",
      "|199.189.88.51 - hettinger7835 [15/Aug/2023:15:32:09 +0700] \"POST /infrastructures HTTP/1.1\" 401 6369                         |\n",
      "|221.64.200.187 - - [15/Aug/2023:15:32:09 +0700] \"DELETE /architectures HTTP/1.0\" 100 9554                                    |\n",
      "|87.253.162.157 - robel3528 [15/Aug/2023:15:32:09 +0700] \"PATCH /virtual/next-generation/roi/transform HTTP/2.0\" 504 19088    |\n",
      "|61.234.128.42 - damore5324 [15/Aug/2023:15:32:09 +0700] \"POST /24%2f365/one-to-one/best-of-breed/solutions HTTP/2.0\" 405 6134|\n",
      "|124.188.77.196 - - [15/Aug/2023:15:32:09 +0700] \"HEAD /vertical/initiatives/efficient/sticky HTTP/1.0\" 201 924               |\n",
      "|187.167.247.10 - bradtke6637 [15/Aug/2023:15:32:09 +0700] \"HEAD /exploit/orchestrate HTTP/1.1\" 416 13584                     |\n",
      "|157.136.96.51 - - [15/Aug/2023:15:32:09 +0700] \"GET /scalable/vertical/platforms/technologies HTTP/2.0\" 100 13832            |\n",
      "|187.217.130.116 - - [15/Aug/2023:15:32:09 +0700] \"PATCH /enterprise HTTP/1.0\" 416 27439                                      |\n",
      "|34.175.63.151 - - [15/Aug/2023:15:32:09 +0700] \"PUT /eyeballs/synergistic/web+services HTTP/2.0\" 403 281                     |\n",
      "|99.205.83.14 - hirthe2077 [15/Aug/2023:15:32:09 +0700] \"HEAD /incubate HTTP/1.0\" 405 15762                                   |\n",
      "|74.49.105.108 - - [15/Aug/2023:15:32:09 +0700] \"GET /out-of-the-box/interfaces/incubate/sexy HTTP/2.0\" 201 26699             |\n",
      "|180.4.82.81 - - [15/Aug/2023:15:32:09 +0700] \"DELETE /matrix HTTP/2.0\" 416 11935                                             |\n",
      "|11.105.184.84 - denesik8504 [15/Aug/2023:15:32:09 +0700] \"POST /evolve/action-items/brand HTTP/2.0\" 200 16257                |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_file_path= \"dataset/apache_common.log\"\n",
    "\n",
    "base_df = spark.read.text(log_file_path)\n",
    "# Let's look at the schema\n",
    "base_df.printSchema()\n",
    "base_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------+--------------------------------------------+------+------------+\n",
      "|host            |timestamp                 |path                                        |status|content_size|\n",
      "+----------------+--------------------------+--------------------------------------------+------+------------+\n",
      "|97.86.147.130   |15/Aug/2023:15:32:09 +0700|/niches                                     |405   |3170        |\n",
      "|162.176.171.13  |15/Aug/2023:15:32:09 +0700|/ubiquitous/empower/content/roi             |502   |10984       |\n",
      "|70.109.63.88    |15/Aug/2023:15:32:09 +0700|/reinvent/innovative                        |304   |5075        |\n",
      "|237.80.86.112   |15/Aug/2023:15:32:09 +0700|/target                                     |100   |24483       |\n",
      "|198.91.2.115    |15/Aug/2023:15:32:09 +0700|/infrastructures                            |503   |24834       |\n",
      "|37.79.127.83    |15/Aug/2023:15:32:09 +0700|/open-source/scale/synergies/engage         |500   |11962       |\n",
      "|28.59.248.3     |15/Aug/2023:15:32:09 +0700|/maximize/b2c                               |302   |20227       |\n",
      "|199.189.88.51   |15/Aug/2023:15:32:09 +0700|/infrastructures                            |401   |6369        |\n",
      "|221.64.200.187  |15/Aug/2023:15:32:09 +0700|/architectures                              |100   |9554        |\n",
      "|87.253.162.157  |15/Aug/2023:15:32:09 +0700|/virtual/next-generation/roi/transform      |504   |19088       |\n",
      "|61.234.128.42   |15/Aug/2023:15:32:09 +0700|/24%2f365/one-to-one/best-of-breed/solutions|405   |6134        |\n",
      "|124.188.77.196  |15/Aug/2023:15:32:09 +0700|/vertical/initiatives/efficient/sticky      |201   |924         |\n",
      "|187.167.247.10  |15/Aug/2023:15:32:09 +0700|/exploit/orchestrate                        |416   |13584       |\n",
      "|157.136.96.51   |15/Aug/2023:15:32:09 +0700|/scalable/vertical/platforms/technologies   |100   |13832       |\n",
      "|187.217.130.116 |15/Aug/2023:15:32:09 +0700|/enterprise                                 |416   |27439       |\n",
      "|34.175.63.151   |15/Aug/2023:15:32:09 +0700|/eyeballs/synergistic/web+services          |403   |281         |\n",
      "|99.205.83.14    |15/Aug/2023:15:32:09 +0700|/incubate                                   |405   |15762       |\n",
      "|74.49.105.108   |15/Aug/2023:15:32:09 +0700|/out-of-the-box/interfaces/incubate/sexy    |201   |26699       |\n",
      "|180.4.82.81     |15/Aug/2023:15:32:09 +0700|/matrix                                     |416   |11935       |\n",
      "|11.105.184.84   |15/Aug/2023:15:32:09 +0700|/evolve/action-items/brand                  |200   |16257       |\n",
      "+----------------+--------------------------+--------------------------------------------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, regexp_extract\n",
    "split_df = base_df.select(regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('host'),\n",
    "                          regexp_extract('value', r'\\[(.*?)\\]', 1).alias('timestamp'),\n",
    "                          regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n",
    "                          regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n",
    "                          regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\n",
    "split_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.filter(base_df['value'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows_df = split_df.filter(split_df['host'].isNull() |\n",
    "                              split_df['timestamp'].isNull() |\n",
    "                              split_df['path'].isNull() |\n",
    "                              split_df['status'].isNull() |\n",
    "                             split_df['content_size'].isNull())\n",
    "bad_rows_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+------+------------+\n",
      "|host|timestamp|path|status|content_size|\n",
      "+----+---------+----+------+------------+\n",
      "|   0|        0|   0|     0|           0|\n",
      "+----+---------+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "def count_null(col_name):\n",
    "  return sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "exprs = []\n",
    "[exprs.append(count_null(col_name)) for col_name in split_df.columns]\n",
    "split_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203025\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "|host            |path                                        |status|content_size|time               |\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "|97.86.147.130   |/niches                                     |405   |3170        |2023-08-15 15:32:09|\n",
      "|162.176.171.13  |/ubiquitous/empower/content/roi             |502   |10984       |2023-08-15 15:32:09|\n",
      "|70.109.63.88    |/reinvent/innovative                        |304   |5075        |2023-08-15 15:32:09|\n",
      "|237.80.86.112   |/target                                     |100   |24483       |2023-08-15 15:32:09|\n",
      "|198.91.2.115    |/infrastructures                            |503   |24834       |2023-08-15 15:32:09|\n",
      "|37.79.127.83    |/open-source/scale/synergies/engage         |500   |11962       |2023-08-15 15:32:09|\n",
      "|28.59.248.3     |/maximize/b2c                               |302   |20227       |2023-08-15 15:32:09|\n",
      "|199.189.88.51   |/infrastructures                            |401   |6369        |2023-08-15 15:32:09|\n",
      "|221.64.200.187  |/architectures                              |100   |9554        |2023-08-15 15:32:09|\n",
      "|87.253.162.157  |/virtual/next-generation/roi/transform      |504   |19088       |2023-08-15 15:32:09|\n",
      "|61.234.128.42   |/24%2f365/one-to-one/best-of-breed/solutions|405   |6134        |2023-08-15 15:32:09|\n",
      "|124.188.77.196  |/vertical/initiatives/efficient/sticky      |201   |924         |2023-08-15 15:32:09|\n",
      "|187.167.247.10  |/exploit/orchestrate                        |416   |13584       |2023-08-15 15:32:09|\n",
      "|157.136.96.51   |/scalable/vertical/platforms/technologies   |100   |13832       |2023-08-15 15:32:09|\n",
      "|187.217.130.116 |/enterprise                                 |416   |27439       |2023-08-15 15:32:09|\n",
      "|34.175.63.151   |/eyeballs/synergistic/web+services          |403   |281         |2023-08-15 15:32:09|\n",
      "|99.205.83.14    |/incubate                                   |405   |15762       |2023-08-15 15:32:09|\n",
      "|74.49.105.108   |/out-of-the-box/interfaces/incubate/sexy    |201   |26699       |2023-08-15 15:32:09|\n",
      "|180.4.82.81     |/matrix                                     |416   |11935       |2023-08-15 15:32:09|\n",
      "|11.105.184.84   |/evolve/action-items/brand                  |200   |16257       |2023-08-15 15:32:09|\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "logs_df = split_df.select('*', to_timestamp(split_df['timestamp'],\"dd/MMM/yyyy:HH:mm:ss ZZZZ\").cast('timestamp').alias('time')).drop('timestamp')\n",
    "total_log_entries = logs_df.count()\n",
    "print(total_log_entries)\n",
    "logs_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser logs from drain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drain3 started with 'FILE' persistence\n",
      "4 masking instructions are in use\n",
      "Starting training mode. Reading from std-in ('q' to finish)\n"
     ]
    }
   ],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "persistence_type = \"FILE\"\n",
    "persistence = FilePersistence(\"drain3_state.bin\")\n",
    "\n",
    "config = TemplateMinerConfig()\n",
    "config.load(\"drain3.ini\")\n",
    "config.profiling_enabled = False\n",
    "\n",
    "template_miner = TemplateMiner(persistence, config)\n",
    "print(f\"Drain3 started with '{persistence_type}' persistence\")\n",
    "print(f\"{len(config.masking_instructions)} masking instructions are in use\")\n",
    "print(f\"Starting training mode. Reading from std-in ('q' to finish)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/apache_error.log\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 126405\n",
      "Number of clusters: 183\n"
     ]
    }
   ],
   "source": [
    "total_lines = 0\n",
    "for line in lines:\n",
    "    total_lines += 1\n",
    "    line = line.rstrip()\n",
    "    split_line = line.split('] ')\n",
    "    message = split_line[4]\n",
    "    template_miner.add_log_message(message)\n",
    "\n",
    "print('Total lines: {}'.format(total_lines))\n",
    "print('Number of clusters: {}'.format(len(template_miner.drain.clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_log =\\\n",
    "\"\"\"\n",
    "We need to compress the wireless HTTP transmitter!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched template #7: We need to <:*:> the <:*:> <:*:> <:*:>\n",
      "Parameters: []\n"
     ]
    }
   ],
   "source": [
    "cluster = template_miner.match(example_log)\n",
    "if cluster is None:\n",
    "        print(f\"No match found\")\n",
    "else:\n",
    "    template = cluster.get_template()\n",
    "    print(f\"Matched template #{cluster.cluster_id}: {template}\")\n",
    "    print(f\"Parameters: {template_miner.get_parameter_list(template, example_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
