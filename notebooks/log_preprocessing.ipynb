{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"pyspark-notebook\").\\\n",
    "config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected                                                          |\n",
      "|- 1117838573 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.53.276129 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected                                                          |\n",
      "|- 1117838976 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.49.36.156884 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected                                                          |\n",
      "|- 1117838978 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.49.38.026704 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected                                                          |\n",
      "|- 1117842440 2005.06.03 R23-M0-NE-C:J05-U01 2005-06-03-16.47.20.730545 R23-M0-NE-C:J05-U01 RAS KERNEL INFO 63543 double-hummer alignment exceptions                                                          |\n",
      "|- 1117842974 2005.06.03 R24-M0-N1-C:J13-U11 2005-06-03-16.56.14.254137 R24-M0-N1-C:J13-U11 RAS KERNEL INFO 162 double-hummer alignment exceptions                                                            |\n",
      "|- 1117843015 2005.06.03 R21-M1-N6-C:J08-U11 2005-06-03-16.56.55.309974 R21-M1-N6-C:J08-U11 RAS KERNEL INFO 141 double-hummer alignment exceptions                                                            |\n",
      "|- 1117848119 2005.06.03 R16-M1-N2-C:J17-U01 2005-06-03-18.21.59.871925 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 2, at 0x0b85eee0, mask 0x05                                                                |\n",
      "|APPREAD 1117869872 2005.06.04 R04-M1-N4-I:J18-U11 2005-06-04-00.24.32.432192 R04-M1-N4-I:J18-U11 RAS APP FATAL ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33569|\n",
      "|APPREAD 1117869876 2005.06.04 R27-M1-N4-I:J18-U01 2005-06-04-00.24.36.222560 R27-M1-N4-I:J18-U01 RAS APP FATAL ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33370|\n",
      "|- 1117942120 2005.06.04 R30-M0-N7-C:J08-U01 2005-06-04-20.28.40.767551 R30-M0-N7-C:J08-U01 RAS KERNEL INFO CE sym 20, at 0x1438f9e0, mask 0x40                                                               |\n",
      "|- 1117955341 2005.06.05 R25-M0-N7-C:J02-U01 2005-06-05-00.09.01.903373 R25-M0-N7-C:J02-U01 RAS KERNEL INFO generating core.2275                                                                              |\n",
      "|- 1117955392 2005.06.05 R24-M1-N8-C:J09-U11 2005-06-05-00.09.52.516674 R24-M1-N8-C:J09-U11 RAS KERNEL INFO generating core.862                                                                               |\n",
      "|- 1117956980 2005.06.05 R24-M1-NB-C:J15-U11 2005-06-05-00.36.20.945796 R24-M1-NB-C:J15-U11 RAS KERNEL INFO generating core.728                                                                               |\n",
      "|- 1117957045 2005.06.05 R20-M1-N8-C:J04-U01 2005-06-05-00.37.25.012681 R20-M1-N8-C:J04-U01 RAS KERNEL INFO generating core.775                                                                               |\n",
      "|- 1117959501 2005.06.05 R24-M0-NE-C:J14-U11 2005-06-05-01.18.21.778604 R24-M0-NE-C:J14-U11 RAS KERNEL INFO generating core.3276                                                                              |\n",
      "|- 1117959513 2005.06.05 R21-M1-N2-C:J11-U01 2005-06-05-01.18.33.830595 R21-M1-N2-C:J11-U01 RAS KERNEL INFO generating core.1717                                                                              |\n",
      "|- 1117959563 2005.06.05 R24-M0-N8-C:J04-U11 2005-06-05-01.19.23.822135 R24-M0-N8-C:J04-U11 RAS KERNEL INFO generating core.3919                                                                              |\n",
      "|- 1117973759 2005.06.05 R31-M0-NE-C:J05-U11 2005-06-05-05.15.59.416717 R31-M0-NE-C:J05-U11 RAS KERNEL INFO generating core.2079                                                                              |\n",
      "|- 1117973786 2005.06.05 R36-M0-NA-C:J06-U01 2005-06-05-05.16.26.686603 R36-M0-NA-C:J06-U01 RAS KERNEL INFO generating core.1414                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_file_path= \"dataset/BGL_2k.log\"\n",
    "\n",
    "base_df = spark.read.text(log_file_path)\n",
    "# Let's look at the schema\n",
    "base_df.printSchema()\n",
    "base_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+------+------------+\n",
      "|host    |timestamp|path|status|content_size|\n",
      "+--------+---------+----+------+------------+\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|APPREAD |         |    |null  |null        |\n",
      "|APPREAD |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "+--------+---------+----+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, regexp_extract\n",
    "split_df = base_df.select(regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('host'),\n",
    "                          regexp_extract('value', r'\\[(.*?)\\]', 1).alias('timestamp'),\n",
    "                          regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n",
    "                          regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n",
    "                          regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\n",
    "split_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.filter(base_df['value'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows_df = split_df.filter(split_df['host'].isNull() |\n",
    "                              split_df['timestamp'].isNull() |\n",
    "                              split_df['path'].isNull() |\n",
    "                              split_df['status'].isNull() |\n",
    "                             split_df['content_size'].isNull())\n",
    "bad_rows_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+------+------------+\n",
      "|host|timestamp|path|status|content_size|\n",
      "+----+---------+----+------+------------+\n",
      "|   0|        0|   0|     0|           0|\n",
      "+----+---------+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "def count_null(col_name):\n",
    "  return sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "exprs = []\n",
    "[exprs.append(count_null(col_name)) for col_name in split_df.columns]\n",
    "split_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203025\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "|host            |path                                        |status|content_size|time               |\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "|97.86.147.130   |/niches                                     |405   |3170        |2023-08-15 15:32:09|\n",
      "|162.176.171.13  |/ubiquitous/empower/content/roi             |502   |10984       |2023-08-15 15:32:09|\n",
      "|70.109.63.88    |/reinvent/innovative                        |304   |5075        |2023-08-15 15:32:09|\n",
      "|237.80.86.112   |/target                                     |100   |24483       |2023-08-15 15:32:09|\n",
      "|198.91.2.115    |/infrastructures                            |503   |24834       |2023-08-15 15:32:09|\n",
      "|37.79.127.83    |/open-source/scale/synergies/engage         |500   |11962       |2023-08-15 15:32:09|\n",
      "|28.59.248.3     |/maximize/b2c                               |302   |20227       |2023-08-15 15:32:09|\n",
      "|199.189.88.51   |/infrastructures                            |401   |6369        |2023-08-15 15:32:09|\n",
      "|221.64.200.187  |/architectures                              |100   |9554        |2023-08-15 15:32:09|\n",
      "|87.253.162.157  |/virtual/next-generation/roi/transform      |504   |19088       |2023-08-15 15:32:09|\n",
      "|61.234.128.42   |/24%2f365/one-to-one/best-of-breed/solutions|405   |6134        |2023-08-15 15:32:09|\n",
      "|124.188.77.196  |/vertical/initiatives/efficient/sticky      |201   |924         |2023-08-15 15:32:09|\n",
      "|187.167.247.10  |/exploit/orchestrate                        |416   |13584       |2023-08-15 15:32:09|\n",
      "|157.136.96.51   |/scalable/vertical/platforms/technologies   |100   |13832       |2023-08-15 15:32:09|\n",
      "|187.217.130.116 |/enterprise                                 |416   |27439       |2023-08-15 15:32:09|\n",
      "|34.175.63.151   |/eyeballs/synergistic/web+services          |403   |281         |2023-08-15 15:32:09|\n",
      "|99.205.83.14    |/incubate                                   |405   |15762       |2023-08-15 15:32:09|\n",
      "|74.49.105.108   |/out-of-the-box/interfaces/incubate/sexy    |201   |26699       |2023-08-15 15:32:09|\n",
      "|180.4.82.81     |/matrix                                     |416   |11935       |2023-08-15 15:32:09|\n",
      "|11.105.184.84   |/evolve/action-items/brand                  |200   |16257       |2023-08-15 15:32:09|\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "logs_df = split_df.select('*', to_timestamp(split_df['timestamp'],\"dd/MMM/yyyy:HH:mm:ss ZZZZ\").cast('timestamp').alias('time')).drop('timestamp')\n",
    "total_log_entries = logs_df.count()\n",
    "print(total_log_entries)\n",
    "logs_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser logs from drain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drain3 started with 'FILE' persistence\n",
      "4 masking instructions are in use\n",
      "Starting training mode. Reading from std-in ('q' to finish)\n"
     ]
    }
   ],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "persistence_type = \"FILE\"\n",
    "persistence = FilePersistence(\"drain3_state.bin\")\n",
    "\n",
    "config = TemplateMinerConfig()\n",
    "config.load(\"drain3.ini\")\n",
    "config.profiling_enabled = False\n",
    "\n",
    "template_miner = TemplateMiner(persistence, config)\n",
    "print(f\"Drain3 started with '{persistence_type}' persistence\")\n",
    "print(f\"{len(config.masking_instructions)} masking instructions are in use\")\n",
    "print(f\"Starting training mode. Reading from std-in ('q' to finish)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/apache_error.log\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: You can't index the port without copying the cross-platform HTTP microchip!\n",
      "Date: Tue Aug 15 15:32:26 2023\n",
      "Total lines: 1\n",
      "Number of clusters: 183\n"
     ]
    }
   ],
   "source": [
    "total_lines = 0\n",
    "for line in lines:\n",
    "    total_lines += 1\n",
    "    line = line.rstrip()\n",
    "    split_line = line.split('] ')\n",
    "    message = split_line[4]\n",
    "    # template_miner.add_log_message(message)\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Date: {split_line[0][1:]}\")\n",
    "    break\n",
    "\n",
    "print('Total lines: {}'.format(total_lines))\n",
    "print('Number of clusters: {}'.format(len(template_miner.drain.clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_log =\\\n",
    "\"\"\"\n",
    "We need to compress the wireless HTTP transmitter!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched template #7: We need to <:*:> the <:*:> <:*:> <:*:>\n",
      "Parameters: []\n"
     ]
    }
   ],
   "source": [
    "cluster = template_miner.match(example_log)\n",
    "if cluster is None:\n",
    "        print(f\"No match found\")\n",
    "else:\n",
    "    template = cluster.get_template()\n",
    "    print(f\"Matched template #{cluster.cluster_id}: {template}\")\n",
    "    print(f\"Parameters: {template_miner.get_parameter_list(template, example_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
