{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "cassandra_host = \"192.168.31.188\"\n",
    "cassandra_user = \"cassandra\"\n",
    "cassandra_pwd  = \"cassandra\"\n",
    "cassandra_port = 9042\n",
    "key_space      = \"loganalysis\"\n",
    "table_name     = \"bgllogs\"\n",
    "kafka_server   = \"192.168.31.188:9092\"\n",
    "kafka_topic    = \"logs\"\n",
    "\n",
    "#Spark Session creation configured to interact with MongoDB\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").\\\n",
    "    config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector-driver_2.12:3.0.0\").\\\n",
    "    config(\"spark.cassandra.connection.host\",cassandra_host).\\\n",
    "    config(\"spark.cassandra.auth.username\",cassandra_user).\\\n",
    "    config(\"spark.cassandra.auth.password\",cassandra_pwd).\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, Row, StringType, IntegerType\n",
    "\n",
    "kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_server,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"startingOffsets\": \"latest\"  # Adjust this as needed\n",
    "}\n",
    "\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_params) \\\n",
    "    .load()\n",
    "\n",
    "kafka_stream = kafka_stream.selectExpr(\n",
    "    \"CAST(value AS STRING) as kafka_message\"\n",
    ")\n",
    "\n",
    "log_stream = kafka_stream.selectExpr(\"get_json_object(kafka_message, '$.message') as message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.file_persistence import FilePersistence\n",
    "from pyspark.sql.functions import udf, unix_timestamp\n",
    "import datetime\n",
    "\n",
    "persistence = FilePersistence(\"drain3_state_bgl.bin\")\n",
    "config = TemplateMinerConfig()\n",
    "config.load(\"drain3.ini\")\n",
    "config.profiling_enabled = False\n",
    "template_miner = TemplateMiner(persistence, config)\n",
    "\n",
    "def inference_line(log_line):\n",
    "    log_line = log_line.rstrip()\n",
    "    line_in_arr = log_line.split(' ')\n",
    "\n",
    "    if len(line_in_arr) < 9: # wrong format\n",
    "        current_timestamp = datetime.datetime.now()\n",
    "        timestamp_string = current_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return Row('timestamp' ,'cluster_id', 'date', 'content', 'cluster_template', 'label', 'prediction')\\\n",
    "            (timestamp_string,-1, \"NA\", log_line, \"NA\", \"NA\", 'Abnormal')\n",
    "\n",
    "    label = line_in_arr[0]\n",
    "    timestamp = line_in_arr[4]\n",
    "    date = line_in_arr[2]\n",
    "    content = ' '.join(line_in_arr[8:])\n",
    "\n",
    "    cluster = template_miner.match(content)\n",
    "\n",
    "    if cluster is None:\n",
    "        cluster_id = -1\n",
    "        cluster_template = \"NA\"\n",
    "        prediction = \"Abnormal\"\n",
    "    else:\n",
    "        cluster_id = cluster.cluster_id\n",
    "        cluster_template = cluster.get_template()\n",
    "        prediction = \"Normal\"\n",
    "\n",
    "\n",
    "    return Row('timestamp' ,'cluster_id', 'date', 'content', 'cluster_template', 'label', 'prediction')\\\n",
    "        (timestamp ,cluster_id, date, content, cluster_template, label, prediction)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"cluster_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"cluster_template\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"prediction\", StringType(), True)\n",
    "])\n",
    "\n",
    "udf_split = udf(inference_line, schema)\n",
    "\n",
    "processed_df = log_stream\\\n",
    "    .withColumn(\"parsed\", udf_split(log_stream[\"message\"]))\\\n",
    "    .select(\"parsed.*\")\\\n",
    "    .withColumn(\"time_added\",unix_timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write stream to console\n",
    "# query = processed_df.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results to cassandra\n",
    "\n",
    "**Guide**\n",
    "\n",
    "```bash\n",
    "\n",
    "docker exec -i -t cassandra bash\n",
    "\n",
    "cqlsh -u cassandra -p cassandra\n",
    "\n",
    "CREATE KEYSPACE IF NOT EXISTS loganalysis WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS loganalysis.bgllogs (\n",
    "    timestamp text,\n",
    "    time_added text,\n",
    "    cluster_id int,\n",
    "    date text,\n",
    "    content text,\n",
    "    cluster_template text,\n",
    "    label text,\n",
    "    prediction text,\n",
    "    PRIMARY KEY (timestamp)\n",
    ");\n",
    "\n",
    "truncate table loganalysis.bgllogs;\n",
    "\n",
    "```\n",
    "\n",
    "List tables\n",
    "\n",
    "```bash\n",
    "describe tables;\n",
    "```\n",
    "\n",
    "Query:\n",
    "\n",
    "```bash\n",
    "select * from loganalysis.bgllogs where prediction='Abnormal' ALLOW FILTERING;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(df, epoch_id):\n",
    "    \"\"\"Writes data to Cassandra and HDFS location\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Streaming Dataframe\n",
    "    epoch_id : int\n",
    "        Unique id for each micro batch/epoch\n",
    "    \"\"\"\n",
    "    df.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table='bgllogs', keyspace='loganalysis')\\\n",
    "        .save() #hot path\n",
    "\n",
    "#Writes streaming dataframe to ForeachBatch console which ingests data to Cassandra\n",
    "processed_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_row) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
