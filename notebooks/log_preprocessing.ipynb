{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/17 08:33:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"pyspark-notebook\").\\\n",
    "config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.527847 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.823719 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.982731 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.131467 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.293532 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.428563 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.601412 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.749199 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.885834 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.041388 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.199063 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.345821 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.493353 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.638135 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.807927 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838572 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.52.951717 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838573 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.53.125780 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "|- 1117838573 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.53.276129 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_file_path= \"../data/raw/BGL_train.log\"\n",
    "\n",
    "base_df = spark.read.text(log_file_path)\n",
    "# Let's look at the schema\n",
    "base_df.printSchema()\n",
    "base_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_SAME_TYPE] Argument `startPos` and `length` should be the same type, got int and NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m timestamp_col \u001b[39m=\u001b[39m split_cols[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m date_col \u001b[39m=\u001b[39m split_cols[\u001b[39m2\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m log_message_col \u001b[39m=\u001b[39m concat_ws(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, split_cols[\u001b[39m7\u001b[39;49m:])  \u001b[39m# Combine the remaining columns into the log message\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Create a new DataFrame with extracted columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m extracted_df \u001b[39m=\u001b[39m base_df\u001b[39m.\u001b[39mselect(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     label_col\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     timestamp_col\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     date_col\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mpp/Documents/log-analytics-system/notebooks/log_preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     log_message_col\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mlog_message\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/data-engineering/lib/python3.9/site-packages/pyspark/sql/column.py:705\u001b[0m, in \u001b[0;36mColumn.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstep \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mslice with step is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubstr(k\u001b[39m.\u001b[39;49mstart, k\u001b[39m.\u001b[39;49mstop)\n\u001b[1;32m    706\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[39mreturn\u001b[39;00m _bin_op(\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)(\u001b[39mself\u001b[39m, k)\n",
      "File \u001b[0;32m~/.conda/envs/data-engineering/lib/python3.9/site-packages/pyspark/sql/column.py:909\u001b[0m, in \u001b[0;36mColumn.substr\u001b[0;34m(self, startPos, length)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mReturn a :class:`Column` which is a substring of the column.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[39m[Row(col='Ali'), Row(col='Bob')]\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(startPos) \u001b[39m!=\u001b[39m \u001b[39mtype\u001b[39m(length):\n\u001b[0;32m--> 909\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    910\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_SAME_TYPE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    911\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39marg_name1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstartPos\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    913\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39marg_name2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    914\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39marg_type1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(startPos)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m    915\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39marg_type2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(length)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m    916\u001b[0m         },\n\u001b[1;32m    917\u001b[0m     )\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(startPos, \u001b[39mint\u001b[39m):\n\u001b[1;32m    919\u001b[0m     jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jc\u001b[39m.\u001b[39msubstr(startPos, length)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_SAME_TYPE] Argument `startPos` and `length` should be the same type, got int and NoneType."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col, split\n",
    "\n",
    "# Split the text into columns based on spaces\n",
    "split_cols = split(base_df[\"value\"], \" \")\n",
    "label_col = split_cols[0]\n",
    "timestamp_col = split_cols[1]\n",
    "date_col = split_cols[2]\n",
    "log_message_col = concat_ws(\" \", split_cols[7:])  # Combine the remaining columns into the log message\n",
    "\n",
    "# Create a new DataFrame with extracted columns\n",
    "extracted_df = base_df.select(\n",
    "    label_col.alias(\"label\"),\n",
    "    timestamp_col.alias(\"timestamp\"), \n",
    "    date_col.alias(\"date\"),\n",
    "    log_message_col.alias(\"log_message\"))\n",
    "\n",
    "\n",
    "extracted_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+------+------------+\n",
      "|host    |timestamp|path|status|content_size|\n",
      "+--------+---------+----+------+------------+\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|APPREAD |         |    |null  |null        |\n",
      "|APPREAD |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "|-       |         |    |null  |null        |\n",
      "+--------+---------+----+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, regexp_extract\n",
    "split_df = base_df.select(regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('host'),\n",
    "                          regexp_extract('value', r'\\[(.*?)\\]', 1).alias('timestamp'),\n",
    "                          regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n",
    "                          regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n",
    "                          regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\n",
    "split_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.filter(base_df['value'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows_df = split_df.filter(split_df['host'].isNull() |\n",
    "                              split_df['timestamp'].isNull() |\n",
    "                              split_df['path'].isNull() |\n",
    "                              split_df['status'].isNull() |\n",
    "                             split_df['content_size'].isNull())\n",
    "bad_rows_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+------+------------+\n",
      "|host|timestamp|path|status|content_size|\n",
      "+----+---------+----+------+------------+\n",
      "|   0|        0|   0|     0|           0|\n",
      "+----+---------+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "def count_null(col_name):\n",
    "  return sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "exprs = []\n",
    "[exprs.append(count_null(col_name)) for col_name in split_df.columns]\n",
    "split_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203025\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "|host            |path                                        |status|content_size|time               |\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "|97.86.147.130   |/niches                                     |405   |3170        |2023-08-15 15:32:09|\n",
      "|162.176.171.13  |/ubiquitous/empower/content/roi             |502   |10984       |2023-08-15 15:32:09|\n",
      "|70.109.63.88    |/reinvent/innovative                        |304   |5075        |2023-08-15 15:32:09|\n",
      "|237.80.86.112   |/target                                     |100   |24483       |2023-08-15 15:32:09|\n",
      "|198.91.2.115    |/infrastructures                            |503   |24834       |2023-08-15 15:32:09|\n",
      "|37.79.127.83    |/open-source/scale/synergies/engage         |500   |11962       |2023-08-15 15:32:09|\n",
      "|28.59.248.3     |/maximize/b2c                               |302   |20227       |2023-08-15 15:32:09|\n",
      "|199.189.88.51   |/infrastructures                            |401   |6369        |2023-08-15 15:32:09|\n",
      "|221.64.200.187  |/architectures                              |100   |9554        |2023-08-15 15:32:09|\n",
      "|87.253.162.157  |/virtual/next-generation/roi/transform      |504   |19088       |2023-08-15 15:32:09|\n",
      "|61.234.128.42   |/24%2f365/one-to-one/best-of-breed/solutions|405   |6134        |2023-08-15 15:32:09|\n",
      "|124.188.77.196  |/vertical/initiatives/efficient/sticky      |201   |924         |2023-08-15 15:32:09|\n",
      "|187.167.247.10  |/exploit/orchestrate                        |416   |13584       |2023-08-15 15:32:09|\n",
      "|157.136.96.51   |/scalable/vertical/platforms/technologies   |100   |13832       |2023-08-15 15:32:09|\n",
      "|187.217.130.116 |/enterprise                                 |416   |27439       |2023-08-15 15:32:09|\n",
      "|34.175.63.151   |/eyeballs/synergistic/web+services          |403   |281         |2023-08-15 15:32:09|\n",
      "|99.205.83.14    |/incubate                                   |405   |15762       |2023-08-15 15:32:09|\n",
      "|74.49.105.108   |/out-of-the-box/interfaces/incubate/sexy    |201   |26699       |2023-08-15 15:32:09|\n",
      "|180.4.82.81     |/matrix                                     |416   |11935       |2023-08-15 15:32:09|\n",
      "|11.105.184.84   |/evolve/action-items/brand                  |200   |16257       |2023-08-15 15:32:09|\n",
      "+----------------+--------------------------------------------+------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "logs_df = split_df.select('*', to_timestamp(split_df['timestamp'],\"dd/MMM/yyyy:HH:mm:ss ZZZZ\").cast('timestamp').alias('time')).drop('timestamp')\n",
    "total_log_entries = logs_df.count()\n",
    "print(total_log_entries)\n",
    "logs_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser logs from drain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drain3 started with 'FILE' persistence\n",
      "4 masking instructions are in use\n",
      "Starting training mode. Reading from std-in ('q' to finish)\n"
     ]
    }
   ],
   "source": [
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "persistence_type = \"FILE\"\n",
    "persistence = FilePersistence(\"drain3_state.bin\")\n",
    "\n",
    "config = TemplateMinerConfig()\n",
    "config.load(\"drain3.ini\")\n",
    "config.profiling_enabled = False\n",
    "\n",
    "template_miner = TemplateMiner(persistence, config)\n",
    "print(f\"Drain3 started with '{persistence_type}' persistence\")\n",
    "print(f\"{len(config.masking_instructions)} masking instructions are in use\")\n",
    "print(f\"Starting training mode. Reading from std-in ('q' to finish)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/apache_error.log\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: You can't index the port without copying the cross-platform HTTP microchip!\n",
      "Date: Tue Aug 15 15:32:26 2023\n",
      "Total lines: 1\n",
      "Number of clusters: 183\n"
     ]
    }
   ],
   "source": [
    "total_lines = 0\n",
    "for line in lines:\n",
    "    total_lines += 1\n",
    "    line = line.rstrip()\n",
    "    split_line = line.split('] ')\n",
    "    message = split_line[4]\n",
    "    # template_miner.add_log_message(message)\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Date: {split_line[0][1:]}\")\n",
    "    break\n",
    "\n",
    "print('Total lines: {}'.format(total_lines))\n",
    "print('Number of clusters: {}'.format(len(template_miner.drain.clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_log =\\\n",
    "\"\"\"\n",
    "We need to compress the wireless HTTP transmitter!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched template #7: We need to <:*:> the <:*:> <:*:> <:*:>\n",
      "Parameters: []\n"
     ]
    }
   ],
   "source": [
    "cluster = template_miner.match(example_log)\n",
    "if cluster is None:\n",
    "        print(f\"No match found\")\n",
    "else:\n",
    "    template = cluster.get_template()\n",
    "    print(f\"Matched template #{cluster.cluster_id}: {template}\")\n",
    "    print(f\"Parameters: {template_miner.get_parameter_list(template, example_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
