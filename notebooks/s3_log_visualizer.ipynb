{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/6rbnzyf91v9d5v8nb46hmn0h0000gn/T/ipykernel_63742/1408401328.py:9: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/var/folders/66/6rbnzyf91v9d5v8nb46hmn0h0000gn/T/ipykernel_63742/1408401328.py:10: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.graph_objs as go\n",
    "from datetime import datetime,timedelta\n",
    "import time\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 00:14:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "cassandra_host = \"192.168.1.66\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-visualization\").\\\n",
    "config(\"spark.jars.packages\",\"com.datastax.spark:spark-cassandra-connector_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector-driver_2.12:3.0.0\").\\\n",
    "config(\"spark.cassandra.connection.host\",cassandra_host).\\\n",
    "config(\"spark.cassandra.auth.username\",\"cassandra\").\\\n",
    "config(\"spark.cassandra.auth.password\",\"cassandra\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 00:14:10 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+----------+-----+----------+----------+\n",
      "|           timestamp|cluster_id|    cluster_template|             content|      date|label|prediction|time_added|\n",
      "+--------------------+----------+--------------------+--------------------+----------+-----+----------+----------+\n",
      "|2005-06-14-10.58....|       176|FATAL program int...|FATAL program int...|2005.06.14|    -|    Normal|1692551196|\n",
      "|2005-06-14-10.58....|        47|FATAL guaranteed ...|FATAL guaranteed ...|2005.06.14|    -|    Normal|1692551196|\n",
      "|2005-06-14-10.58....|       176|FATAL program int...|FATAL program int...|2005.06.14|    -|    Normal|1692551196|\n",
      "|2005-06-14-10.58....|        47|FATAL guaranteed ...|FATAL guaranteed ...|2005.06.14|    -|    Normal|1692551196|\n",
      "|2005-06-14-10.58....|       176|FATAL program int...|FATAL program int...|2005.06.14|    -|    Normal|1692551196|\n",
      "+--------------------+----------+--------------------+--------------------+----------+-----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logs_df = spark\\\n",
    "             .read\\\n",
    "             .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "             .options(table=\"bgllogs\", keyspace=\"loganalysis\")\\\n",
    "             .load()\n",
    "logs_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 00:18:13 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------------+-------+----+-----+----------+----------+\n",
      "|timestamp|cluster_id|cluster_template|content|date|label|prediction|time_added|\n",
      "+---------+----------+----------------+-------+----+-----+----------+----------+\n",
      "+---------+----------+----------------+-------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter label != '-'\n",
    "logs_df.filter(logs_df.prediction == 'Abnormal').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Location(id='url', refresh=False),\n",
    "    html.Div(id='page-content')\n",
    "], style={'textAlign': 'center'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color assignment\n",
    "colors = {\n",
    "    'background': 'white',#'#0C0F0A',\n",
    "    'text': '#FFFFFF'\n",
    "}\n",
    "\n",
    "def create_header(title):\n",
    "    header_style = {\n",
    "        'background-color' : '#1B95E0',\n",
    "        'padding' : '1.5rem',\n",
    "        'color': 'white',\n",
    "        'font-family': 'Verdana, Geneva, sans-serif'\n",
    "    }\n",
    "    header = html.Header(html.H1(children=title, style=header_style))\n",
    "    return header\n",
    "\n",
    "def generate_table(df, max_rows=10):\n",
    "    table = html.Table(className=\"responsive-table\",\n",
    "                      children=[\n",
    "                          html.Thead(\n",
    "                              html.Tr(\n",
    "                                  children=[html.Th(col.title()) for col in df.columns.values]\n",
    "                                  \n",
    "                                  ),style={'border':'1px black solid'}\n",
    "                              ),\n",
    "                          html.Tbody(\n",
    "                              [\n",
    "                              html.Tr(\n",
    "                                  children=[html.Td(data) for data in d]\n",
    "                                  )\n",
    "                               for d in df.values.tolist()],style={'border':'1px black solid'})\n",
    "                          ]\n",
    "                       , style={'marginLeft': 'auto', 'marginRight': 'auto'}\n",
    "    )\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_dashboard = html.Div(style={'backgroundColor': colors['background']}, children=\n",
    "    [   \n",
    "        html.Div([create_header('Log Analysis - Realtime Dashboard')]),\n",
    "        html.Div([dcc.Graph(id='live-graph', animate=False)] ,style={'width': '100%', 'display': 'inline-block'}),\n",
    "        html.Div([html.H2(\"Top Paths\"), html.Div(id=\"top-paths-table\")],style={'width': '50%', 'display': 'inline-block', 'border':'2px black solid'}),\n",
    "        ##Intervals define the frequency in which the html element should be updated\n",
    "        dcc.Interval(id='graph-update',interval=60*1000, n_intervals=0),\n",
    "        html.Div(id='real-time-content'),\n",
    "        html.Br(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cassandra(filter_condition,limit=False):\n",
    "    logs_df = spark\\\n",
    "             .read\\\n",
    "             .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "             .options(table=\"bgllogs\", keyspace=\"loganalysis\")\\\n",
    "             .load()\\\n",
    "             .filter(filter_condition)\n",
    "    if limit:\n",
    "        return logs_df.limit(5).toPandas()\n",
    "    else:\n",
    "        return logs_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 00:32:35 WARN V2ScanPartitioningAndOrdering: Spark ignores the partitioning CassandraPartitioning. Please use KeyGroupedPartitioning for better performance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>cluster_template</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>time_added</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, cluster_id, cluster_template, content, date, label, prediction, time_added]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_condition = \"prediction == 'Abnormal'\"\n",
    "read_cassandra(filter_condition).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(Output('top-paths-table', 'children'),\n",
    "              Input('graph-update', 'n_intervals')\n",
    "             )\n",
    "def update_top_urls(n_intervals):\n",
    "    try:\n",
    "        processed_time = 0\n",
    "        filter_condition = \"prediction == 'Abnormal' and time_added >'\"+str(processed_time)+\"'\"\n",
    "        df = read_cassandra(filter_condition)\n",
    "        processed_time = time.time()-60\n",
    "\n",
    "        df = df[['content','label', 'prediction']]\n",
    "\n",
    "        return generate_table(df, max_rows=5)\n",
    "    except Exception as e:\n",
    "        #File to capture exceptions\n",
    "        with open('table_errors.txt','a') as f:\n",
    "            f.write(str(e))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://0.0.0.0:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x12c57fa60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, use_reloader=False, port=8050,host= '0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
